Epoch 1/30, Train Loss: 1.1707, Train Accuracy: 55.56%, Validation Loss: 2.6566, Validation Accuracy: 35.13%
Epoch 2/30, Train Loss: 0.9687, Train Accuracy: 63.64%, Validation Loss: 1.9335, Validation Accuracy: 38.86%
Epoch 3/30, Train Loss: 0.8397, Train Accuracy: 68.46%, Validation Loss: 2.0166, Validation Accuracy: 42.72%
Epoch 4/30, Train Loss: 0.7443, Train Accuracy: 71.71%, Validation Loss: 0.6943, Validation Accuracy: 73.02%
Epoch 5/30, Train Loss: 0.6851, Train Accuracy: 73.55%, Validation Loss: 0.7151, Validation Accuracy: 73.38%
Epoch 6/30, Train Loss: 0.6382, Train Accuracy: 75.45%, Validation Loss: 0.7048, Validation Accuracy: 73.85%
Epoch 7/30, Train Loss: 0.6000, Train Accuracy: 76.66%, Validation Loss: 0.8323, Validation Accuracy: 69.31%
Epoch 8/30, Train Loss: 0.5547, Train Accuracy: 78.56%, Validation Loss: 1.5397, Validation Accuracy: 53.71%
Epoch 9/30, Train Loss: 0.5288, Train Accuracy: 79.70%, Validation Loss: 1.2139, Validation Accuracy: 61.69%
Epoch 10/30, Train Loss: 0.4949, Train Accuracy: 81.34%, Validation Loss: 2.3624, Validation Accuracy: 41.02%
Epoch 11/30, Train Loss: 0.4541, Train Accuracy: 83.33%, Validation Loss: 0.6027, Validation Accuracy: 78.73%
Epoch 12/30, Train Loss: 0.4073, Train Accuracy: 85.22%, Validation Loss: 0.7760, Validation Accuracy: 73.11%
Epoch 13/30, Train Loss: 0.3767, Train Accuracy: 86.57%, Validation Loss: 0.6274, Validation Accuracy: 79.07%
Epoch 14/30, Train Loss: 0.3515, Train Accuracy: 87.42%, Validation Loss: 0.7295, Validation Accuracy: 76.57%
Epoch 15/30, Train Loss: 0.3267, Train Accuracy: 88.47%, Validation Loss: 0.7428, Validation Accuracy: 75.13%
Epoch 16/30, Train Loss: 0.3142, Train Accuracy: 88.88%, Validation Loss: 0.3720, Validation Accuracy: 86.75%
Epoch 17/30, Train Loss: 0.2921, Train Accuracy: 89.45%, Validation Loss: 1.5296, Validation Accuracy: 68.79%
Epoch 18/30, Train Loss: 0.2714, Train Accuracy: 90.42%, Validation Loss: 0.5691, Validation Accuracy: 80.56%
Epoch 19/30, Train Loss: 0.2554, Train Accuracy: 91.04%, Validation Loss: 0.3613, Validation Accuracy: 87.61%
Epoch 20/30, Train Loss: 0.2401, Train Accuracy: 91.64%, Validation Loss: 0.5530, Validation Accuracy: 81.52%
Epoch 21/30, Train Loss: 0.2243, Train Accuracy: 92.13%, Validation Loss: 1.8656, Validation Accuracy: 58.66%
Epoch 22/30, Train Loss: 0.2081, Train Accuracy: 92.77%, Validation Loss: 0.8311, Validation Accuracy: 79.00%
Epoch 23/30, Train Loss: 0.1961, Train Accuracy: 93.09%, Validation Loss: 0.7774, Validation Accuracy: 80.89%
Epoch 24/30, Train Loss: 0.1823, Train Accuracy: 93.69%, Validation Loss: 1.1763, Validation Accuracy: 75.49%
Epoch 25/30, Train Loss: 0.1745, Train Accuracy: 93.97%, Validation Loss: 0.3852, Validation Accuracy: 87.67%
Epoch 26/30, Train Loss: 0.1610, Train Accuracy: 94.40%, Validation Loss: 0.5719, Validation Accuracy: 83.38%
Epoch 27/30, Train Loss: 0.1507, Train Accuracy: 94.82%, Validation Loss: 1.1235, Validation Accuracy: 76.55%
Epoch 28/30, Train Loss: 0.1421, Train Accuracy: 95.13%, Validation Loss: 0.3989, Validation Accuracy: 88.00%
Epoch 29/30, Train Loss: 0.1303, Train Accuracy: 95.49%, Validation Loss: 0.9282, Validation Accuracy: 77.27%
Epoch 30/30, Train Loss: 0.1317, Train Accuracy: 95.52%, Validation Loss: 0.6529, Validation Accuracy: 82.92%




class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.dropout = nn.Dropout(0.5)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 9)
        self.attention = nn.Sequential(
            nn.Linear(64 * 7 * 7, 49),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.max_pool2d(x, 2)
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.max_pool2d(x, 2)

        # Flatten the tensor
        x = x.view(x.size(0), -1)

        # Attention mechanism
        attention_weights = self.attention(x).view(x.size(0), 1, 7, 7)
        x = x.view(x.size(0), 64, 7, 7) * attention_weights

        # Flatten again to feed into the dense layer
        x = x.view(x.size(0), -1)

        x = self.dropout(x)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = SimpleCNN()