\section{Discussion}\label{discussion}

\subsection{SimpleCNN}\label{SimpleCNNDiscussion}
In our initial experiments with the MNIST dataset using the SimpleCNN model, we compared the effectiveness of the ReLU and LeakyReLU activation functions. We found no significant difference in performance between them, which led us to continue with ReLU due to the simplicity of our model. For the PathMNIST challenge, our experiments with SimpleCNN showed that using only batch normalisation yielded slightly better results than combining it with dropout layers, suggesting that the latter might introduce unnecessary complexity that reduces model performance in this particular task.

\subsection{ResNet}\label{ResNetDiscussion}
During the training we observed a slight but consistent difference between the validation accuracy after training and the final accuracy on our unlabeled dataset for Kaggle. One idea is, that the dataset used for our submission has more instances of the problematic classes our models struggled with. Over all we managed to improve the performance of our models by a lot, when compared to our first attempts. For the future we should put bigger focus on reproducibility and consistency when doing multiple training runs to test differences between hyperparameters. Our choice of optimizer here, while delivering good results, might have been just a statistical anomaly during testing and it would have made sense to do more in-depth testing on different optimizers. One other approach that might be worth looking in to is to minimize the non-deterministic behaviour duric training with cuda. One way would be to set CuDNN in PyTorch to deterministic mode. Here it would be interesting to see how much of an performance impact this has on the training. We also observed that deeper models would take many more epochs than smaller models until they converged. Some instances of ResNet152 trained for 80 epochs, before early stopping was triggered. Here, the question could be raised how important the gained improvement is, compared to smaller models. It would also be interesting to see how these performance differences would change with larger datasets. Another path that we would like to look into more are ensemble methods. A different implementation, or just the combination of more models of different architectures with our current approach might lead to even better results. 

\subsection{Xception}\label{XceptionDiscussion}
Using the Xception architecture for the PathMNIST dataset significantly improved the results, demonstrating the value of deeper, more complex models for processing complicated datasets. However, this came at the expense of computational resources, emphasising the need to strike a balance between the complexity of the model and the practicalities of computational effort. This experience highlighted the critical trade-offs in deep learning between achieving high accuracy and coping with resource constraints, particularly in the context of complex medical imaging tasks.