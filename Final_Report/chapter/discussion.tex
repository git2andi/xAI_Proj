\section{Discussion}\label{discussion}

\subsection{SimpleCNN}\label{SimpleCNNDiscussion}
In our initial experiments with the MNIST dataset using the SimpleCNN model, we compared the effectiveness of the ReLU and LeakyReLU activation functions. We found no significant difference in performance between them, which led us to continue with ReLU due to the simplicity of our model. For the PathMNIST challenge, our experiments with SimpleCNN showed that using only batch normalisation yielded slightly better results than combining it with dropout layers, suggesting that the latter might introduce unnecessary complexity that reduces model performance in this particular task.

\subsection{AlexNet}\label{AlexNetDiscussion}
Using AlexNet resembled a seemingly natural transition to progress from one of the most simple CNN architectures to using an early deep learning model which can also be loaded with pretrained weights. AlexNet allowed to test various hypotheses and visualise the results.Having hypothesised that balancing out the class distribution of the data would result in better accuracy, the results of the experiment supported that hypothesis. Mixed results however were obtained when testing whether the chosen data augmentation techniques would yield more true positives. It can be speculated that the data augmentation possibly lead to the learning of wrong features. Especially when dealing with medical data, any augmentation applied to the data should be first thoroughly thought through, as introducing features which do not exist in the medical data could lead to putting human life at risk. Finally, applying batch normalisation to AlexNet, no clear effect could be observed. A possible explanation for why batch normalisation didn't really affect the results for AlexNet is that AlexNet is too shallow of a network. Compared to other, deeper networks like different Resnet variants or Xception, AlexNet performed noticeably worse, commonly achieving accuracies around 90\%.




\subsection{ResNet}\label{ResNetDiscussion}
During the training we observed a slight but consistent difference between the validation accuracy after training and the final accuracy on our unlabeled dataset for Kaggle. One idea is, that the dataset used for our submission has more instances of the problematic classes our models struggled with. Over all we managed to improve the performance of our models by a lot, when compared to our first attempts. For the future we should put bigger focus on reproducibility and consistency when doing multiple training runs to test differences between hyperparameters. Our choice of optimizer here, while delivering good results, might have been just a statistical anomaly during testing and it would have made sense to do more in-depth testing on different optimizers. One other approach that might be worth looking in to is to minimize the non-deterministic behaviour duric training with cuda. One way would be to set CuDNN in PyTorch to deterministic mode. Here it would be interesting to see how much of an performance impact this has on the training. We also observed that deeper models would take many more epochs than smaller models until they converged. Some instances of ResNet152 trained for 80 epochs, before early stopping was triggered. Here, the question could be raised how important the gained improvement is, compared to smaller models. It would also be interesting to see how these performance differences would change with larger datasets. Another path that we would like to look into more are ensemble methods. A different implementation, or just the combination of more models of different architectures with our current approach might lead to even better results. 

\subsection{Xception}\label{XceptionDiscussion}
Using the Xception architecture for the PathMNIST dataset significantly improved the results compared to our initial SimpleCNN, demonstrating the value of deeper, more complex models for processing complicated datasets. However, this came at the expense of computational resources, emphasising the need to strike a balance between the complexity of the model and the practicalities of computational effort. This experience highlighted the critical trade-offs in deep learning between achieving high accuracy and coping with resource constraints, particularly in the context of complex medical imaging tasks.