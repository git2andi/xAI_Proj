\section{Appendix}

Initial version of our SimpleCNN, including two convulutional layers:\@

\begin{minted}[mathescape, linenos, fontsize=\scriptsize]{python}
    class SimpleCNN(nn.Module):
        def __init__(self, num_classes=10):
            super(SimpleCNN, self).__init__()
            self.conv1 = nn.Sequential(
                nn.Conv2d(
                    in_channels = 1,
                    out_channels = 32,
                    kernel_size = 5,
                    stride=1,
                    padding="same"
                ),
                nn.LeakyReLU(),
                nn.MaxPool2d(kernel_size=2),
            )
            self.conv2 = nn.Sequential(
                nn.Conv2d(32,64,5,1,"same"),
                nn.LeakyReLU(),
                nn.MaxPool2d(kernel_size=2),
            )
            self.out = nn.Linear(64*7*7, num_classes)

        def forward(self, x):
            x = self.conv1(x)
            x = self.conv2(x)
            x = x.view(-1, 64*7*7)
            output = self.out()
            return torch.log_softmax(output, dim=1)
\end{minted}


Structure of the improved version of the SimpleCNN using three convolutional layers, Batch normalization and Dropout:\@

\begin{minted}[mathescape, linenos, fontsize=\scriptsize]{python}
    class SimpleCNN(nn.Module):
        def __init__(self, num_classes=10):
            super(SimpleCNN, self).__init__()
            self.conv1 = nn.Sequential(
                nn.Conv2d(3, 32, kernel_size=3, stride=1, padding="same"),
                nn.BatchNorm2d(32),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2),
                nn.Dropout(0.25)
            )
            self.conv2 = nn.Sequential(
                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding="same"),
                nn.BatchNorm2d(64),            
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Dropout(0.25)
            )
            self.conv3 = nn.Sequential(
                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding="same"),
                nn.BatchNorm2d(128),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Dropout(0.25)
            )
            self.fc1 = nn.Linear(128 * 3 * 3, 256)
            self.fc_bn = nn.BatchNorm1d(256)
            self.dropout_fc = nn.Dropout(0.5)
            self.fc2 = nn.Linear(256, num_classes)
        
        def forward(self, x):
            x = self.conv1(x)
            x = self.conv2(x)
            x = self.conv3(x)
            x = x.view(-1, 128 * 3 * 3)
            x = F.relu(self.fc_bn(self.fc1(x)))
            x = self.dropout_fc(x)
            x = self.fc2(x)
            return torch.log_softmax(x, dim=1)
\end{minted}
    